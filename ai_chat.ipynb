{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 98,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Elements loader\n",
    "\n",
    "from langchain.document_loaders import UnstructuredWordDocumentLoader\n",
    "from langchain.document_loaders import DirectoryLoader\n",
    "\n",
    "elements_loader = DirectoryLoader(path = \"docx_docs\", \n",
    "                                  loader_cls = UnstructuredWordDocumentLoader,\n",
    "                                  loader_kwargs = {'mode' : \"elements\", 'strategy': \"fast\"},\n",
    "                                  recursive = True)\n",
    "\n",
    "docs_elements = elements_loader.load()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 99,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Recursive loaders\n",
    "\n",
    "from langchain.text_splitter import RecursiveCharacterTextSplitter\n",
    "from langchain.document_loaders import UnstructuredWordDocumentLoader\n",
    "from langchain.document_loaders import DirectoryLoader\n",
    "\n",
    "single_loader = DirectoryLoader(path = \"docx_docs\", \n",
    "                                loader_cls = UnstructuredWordDocumentLoader,\n",
    "                                loader_kwargs = {'mode' : \"single\", 'strategy': \"fast\"},\n",
    "                                recursive = True)\n",
    "docs_single = single_loader.load()\n",
    "\n",
    "text_splitter = RecursiveCharacterTextSplitter(chunk_size = 2000, chunk_overlap  = 0, separators = [\"\\n\\n\", \"(?<=\\n)\", \"(?<=\\. )\"], is_separator_regex = True)\n",
    "child_text_splitter = RecursiveCharacterTextSplitter(chunk_size = 500, chunk_overlap  = 0, separators = [\"\\n\\n\", \"(?<=\\n)\", \"(?<=\\. )\"], is_separator_regex = True)\n",
    "\n",
    "docs_recursive = text_splitter.split_documents(docs_single)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 100,
   "metadata": {},
   "outputs": [],
   "source": [
    "# NLTK loader\n",
    "from langchain.text_splitter import NLTKTextSplitter\n",
    "\n",
    "nltk_splitter = NLTKTextSplitter()\n",
    "\n",
    "docs_nltk = nltk_splitter.split_documents(docs_single)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 101,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Finename metadata rename \n",
    "\n",
    "for doc in docs_nltk:\n",
    "    doc.metadata['filename'] = doc.metadata['source'].split('/')[-1]\n",
    "    del doc.metadata['source']\n",
    "\n",
    "for doc in docs_recursive:\n",
    "    doc.metadata['filename'] = doc.metadata['source'].split('/')[-1]\n",
    "    del doc.metadata['source']\n",
    "\n",
    "for doc in docs_single:\n",
    "    doc.metadata['filename'] = doc.metadata['source'].split('/')[-1]\n",
    "    del doc.metadata['source']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 102,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Remove undeeded metadata from Elements documents\n",
    "\n",
    "keys_to_delete = ['source', 'file_directory', 'last_modified', 'filetype', 'primary', 'text_as_html', 'emphasized_text_tags', 'emphasized_text_contents']\n",
    "\n",
    "for doc in docs_elements:\n",
    "    for key in keys_to_delete:\n",
    "        if key in doc.metadata.keys():\n",
    "            del doc.metadata[key]\n",
    "\n",
    "# Removing unneeded documents\n",
    "categories_to_remove = ['PageBreak', 'ListItem', 'Footer', 'Table', 'UncategorizedText', 'Header']\n",
    "docs_elements = [doc for doc in docs_elements if doc.metadata['category'] not in categories_to_remove]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 103,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plots\n",
    "\n",
    "# from matplotlib import pyplot as plt\n",
    "\n",
    "# plt.figure(1)\n",
    "# plt.hist([len(d.page_content) for d in docs_nltk], bins = 100)\n",
    "# plt.grid()\n",
    "# plt.title(\"NLTK документи\")\n",
    "# plt.xlabel(\"број карактера у документу\")\n",
    "# plt.ylabel(\"број докумената\")\n",
    "# plt.figure(2)\n",
    "# plt.hist([len(d.page_content) for d in docs_elements], bins = 100)\n",
    "# plt.grid()\n",
    "# plt.title(\"Elements documents\")\n",
    "# plt.xlabel(\"број карактера у документу\")\n",
    "# plt.ylabel(\"број докумената\")\n",
    "# plt.figure(3)\n",
    "# plt.hist([len(d.page_content) for d in docs_recursive], bins = 100)\n",
    "# plt.grid()\n",
    "# plt.title(\"Recursive documents\")\n",
    "# plt.xlabel(\"број карактера у документу\")\n",
    "# plt.ylabel(\"број докумената\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 104,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Metadata parsing\n",
    "\n",
    "import json\n",
    "\n",
    "metadata_dict = {}\n",
    "metadatas = json.load(open('metadata.json'))\n",
    "\n",
    "ascii_replace_dict = {'â€™': '’', 'â€œ': '“', 'â€': '”'}\n",
    "def replace_non_ascii(s):\n",
    "    for pair_k, pair_v in ascii_replace_dict.items():\n",
    "        s = s.replace(pair_k, pair_v)\n",
    "    return s\n",
    "\n",
    "for metadata in metadatas[2]['data']:\n",
    "    if metadata['id'] in ['228', '413']:\n",
    "        # These are duplicates, not needed\n",
    "        continue\n",
    "    key = ''.join(metadata['file_url'].split('/')[-1].rsplit('.', 1)[:-1])\n",
    "\n",
    "    # Repalce non-ascii characters in key\n",
    "    key = replace_non_ascii(key)\n",
    "    \n",
    "    # Make sure that there are no duplicate entries for the same document type\n",
    "    if (key in metadata_dict.keys() and metadata['type'] == metadata_dict[key]['type']):\n",
    "        print(key)\n",
    "        print(metadata['id'])\n",
    "    assert not (key in metadata_dict.keys() and metadata['type'] == metadata_dict[key]['type'])\n",
    "    metadata_dict[key] = {key: value for key, value in metadata.items() if value is not None}\n",
    "    \n",
    "    # Replace non-ascii characters in file url\n",
    "    del metadata_dict[key]['file_url']\n",
    "    for m_key, m_value in metadata_dict.items():\n",
    "        if m_value is None:\n",
    "            del metadata_dict[m_key]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 105,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Date parsing\n",
    "\n",
    "for metadata_key, metadata_value in metadata_dict.items():\n",
    "    date = metadata_value['date'].split('-')\n",
    "    metadata_value['year'] = int(date[0])\n",
    "    metadata_value['month'] = int(date[1])\n",
    "    metadata_value['day'] = int(date[2])\n",
    "    del metadata_value['date']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 106,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Adding metadata\n",
    "\n",
    "for doc in docs_elements:\n",
    "    assert doc.metadata['filename'][:-5] in metadata_dict.keys()\n",
    "    doc.metadata.update(metadata_dict[doc.metadata['filename'][:-5]])\n",
    "\n",
    "for doc in docs_nltk:\n",
    "    assert doc.metadata['filename'][:-5] in metadata_dict.keys()\n",
    "    doc.metadata.update(metadata_dict[doc.metadata['filename'][:-5]])\n",
    "\n",
    "for doc in docs_recursive:\n",
    "    assert doc.metadata['filename'][:-5] in metadata_dict.keys()\n",
    "    doc.metadata.update(metadata_dict[doc.metadata['filename'][:-5]])\n",
    "\n",
    "for doc in docs_single:\n",
    "    assert doc.metadata['filename'][:-5] in metadata_dict.keys()\n",
    "    doc.metadata.update(metadata_dict[doc.metadata['filename'][:-5]])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 107,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Summary document\n",
    "\n",
    "file_path = 'summary.txt'\n",
    "\n",
    "with open(file_path, 'w') as file:\n",
    "    file.write('This file is a summary document of all of the articles, lectures and patents\\n')\n",
    "    file.write('\\nArticles:\\n')\n",
    "    file.write('\\nLectures:\\n')\n",
    "    file.write('\\nPatents:\\n')\n",
    "\n",
    "a_num = 0\n",
    "l_num = 0\n",
    "p_num = 0\n",
    "\n",
    "for metadata_key in metadata_dict.keys():\n",
    "    with open(file_path, 'r') as file:\n",
    "        lines = file.readlines()\n",
    "\n",
    "    new_lines = []\n",
    "\n",
    "    a_num += 1 if metadata_dict[metadata_key]['type'] == 'article' else 0\n",
    "    l_num += 1 if metadata_dict[metadata_key]['type'] == 'lecture' else 0\n",
    "    p_num += 1 if metadata_dict[metadata_key]['type'] == 'patent' else 0\n",
    "\n",
    "    for i, line in enumerate(lines):\n",
    "        new_lines.append(line)\n",
    "        \n",
    "        if metadata_dict[metadata_key]['type'] == 'article':\n",
    "            if 'Articles:' in line:\n",
    "                new_line = f\"    - Article {metadata_dict[metadata_key]['title']} written in {metadata_dict[metadata_key]['year']} published by {metadata_dict[metadata_key]['source']}\\n\"\n",
    "                new_lines.append(new_line)\n",
    "        \n",
    "        if metadata_dict[metadata_key]['type'] == 'lecture':\n",
    "            if 'Lectures:' in line:\n",
    "                new_line = f\"    - Lecture {metadata_dict[metadata_key]['title']} held in {metadata_dict[metadata_key]['year']}\\n\"\n",
    "                new_lines.append(new_line)\n",
    "        \n",
    "        if metadata_dict[metadata_key]['type'] == 'patent':\n",
    "            if 'Patents:' in line:\n",
    "                new_line = f\"    - Patent {metadata_dict[metadata_key]['title']} filed in {metadata_dict[metadata_key]['year']} with registration number {metadata_dict[metadata_key]['register_num']}\\n\"\n",
    "                new_lines.append(new_line)\n",
    "\n",
    "\n",
    "    with open(file_path, 'w') as file:\n",
    "        file.writelines(new_lines)\n",
    "\n",
    "    \n",
    "with open(file_path, 'r') as file:\n",
    "    lines = file.readlines()\n",
    "new_lines = []\n",
    "for i, line in enumerate(lines):\n",
    "    if 'Patents:' in line:\n",
    "        new_lines.append(f\"Patents ({p_num} files):\\n\")\n",
    "    elif 'Articles:' in line:\n",
    "        new_lines.append(f\"Articles ({a_num} files):\\n\")\n",
    "    elif 'Lectures:' in line:\n",
    "        new_lines.append(f\"Lectures ({l_num} files):\\n\")\n",
    "    else:\n",
    "        new_lines.append(line)\n",
    "with open(file_path, 'w') as file:\n",
    "    file.writelines(new_lines)\n",
    "\n",
    "from langchain.document_loaders import TextLoader\n",
    "\n",
    "doc = TextLoader(file_path).load()\n",
    "\n",
    "docs_metadata = text_splitter.split_documents(doc)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Vectorstore creation\n",
    "\n",
    "from langchain.vectorstores import Chroma\n",
    "from langchain.embeddings import OpenAIEmbeddings\n",
    "\n",
    "embeddings = OpenAIEmbeddings()\n",
    "\n",
    "# vectorstore_elements = Chroma.from_documents(\n",
    "#     collection_name=\"elements\",\n",
    "#     documents = docs_elements + docs_metadata,\n",
    "#     embedding = embeddings,\n",
    "#     persist_directory=\"./vectorstore_elements\"\n",
    "# )\n",
    "\n",
    "# vectorstore_nltk = Chroma.from_documents(\n",
    "#     collection_name=\"nltk\",\n",
    "#     documents = docs_nltk + docs_metadata,\n",
    "#     embedding = embeddings,\n",
    "#     persist_directory=\"./vectorstore_nltk\"\n",
    "# )\n",
    "\n",
    "# vectorstore_recursive = Chroma.from_documents(\n",
    "#     collection_name=\"recursive\",\n",
    "#     documents = docs_recursive + docs_metadata,\n",
    "#     embedding = embeddings,\n",
    "#     persist_directory=\"./vectorstore_recursive\"\n",
    "# )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 110,
   "metadata": {},
   "outputs": [],
   "source": [
    "# LLM loading\n",
    "\n",
    "from langchain.chat_models import ChatOpenAI\n",
    "\n",
    "llm_name = \"gpt-3.5-turbo\"\n",
    "llm = ChatOpenAI(model_name = llm_name, temperature = 0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 111,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Vectorstore loading\n",
    "\n",
    "from langchain.vectorstores import Chroma\n",
    "from langchain.embeddings import OpenAIEmbeddings\n",
    "\n",
    "embeddings = OpenAIEmbeddings()\n",
    "\n",
    "names = ['elements', 'nltk', 'recursive']\n",
    "all_bases = {}\n",
    "all_retrievers = {}\n",
    "\n",
    "for name in names:\n",
    "    all_bases[name] = Chroma(persist_directory=f\"./vectorstore_{name}\", embedding_function = embeddings)\n",
    "    all_retrievers[name] = {}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 112,
   "metadata": {},
   "outputs": [],
   "source": [
    "# BaseRetriever\n",
    "\n",
    "for name in names:\n",
    "    all_retrievers[name]['base'] = all_bases[name].as_retriever(search_type = 'mmr', \n",
    "                                                                search_kwargs = {'k': 3})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 113,
   "metadata": {},
   "outputs": [],
   "source": [
    "# MultiQueryRetriever\n",
    "\n",
    "from langchain.retrievers.multi_query import MultiQueryRetriever\n",
    "\n",
    "for name in names:\n",
    "    all_retrievers[name]['multi_query'] = MultiQueryRetriever.from_llm(retriever = all_retrievers[name]['base'], \n",
    "                                                                       llm = llm)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 114,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ContextualCompressionRetriever\n",
    "\n",
    "from langchain.retrievers import ContextualCompressionRetriever\n",
    "from langchain.retrievers.document_compressors import LLMChainExtractor\n",
    "\n",
    "for name in names:\n",
    "    compressor = LLMChainExtractor.from_llm(llm)\n",
    "    all_retrievers[name]['compression_retriever'] = ContextualCompressionRetriever(base_compressor = compressor, \n",
    "                                                                                   base_retriever = all_retrievers[name]['base'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 125,
   "metadata": {},
   "outputs": [],
   "source": [
    "# SelfQueryRetriever\n",
    "\n",
    "from langchain.chains.query_constructor.base import AttributeInfo\n",
    "from langchain.retrievers.self_query.base import SelfQueryRetriever\n",
    "\n",
    "metadata_field_info = [\n",
    "    AttributeInfo(\n",
    "        name=\"category\",\n",
    "        description=\"Category of te text content - possible values are NarrativeText and Title\",\n",
    "        type=\"string\",\n",
    "    ),\n",
    "    AttributeInfo(\n",
    "        name=\"year\",\n",
    "        description=\"Year when the document was published\",\n",
    "        type=\"integer\",\n",
    "    ),\n",
    "    AttributeInfo(\n",
    "        name=\"day\",\n",
    "        description=\"Day when the document was published\",\n",
    "        type=\"integer\",\n",
    "    ),\n",
    "    AttributeInfo(\n",
    "        name=\"month\",\n",
    "        description=\"Month when the document was published\",\n",
    "        type=\"integer\",\n",
    "    ),\n",
    "    AttributeInfo(\n",
    "        name=\"filename\",\n",
    "        description=\"Name of the file\",\n",
    "        type=\"string\",\n",
    "    ),\n",
    "    AttributeInfo(\n",
    "        name=\"id\",\n",
    "        description=\"Document ID\",\n",
    "        type=\"string\",\n",
    "    ),\n",
    "    AttributeInfo(\n",
    "        name=\"page_number\",\n",
    "        description=\"Page number from the original document\",\n",
    "        type=\"string\",\n",
    "    ),\n",
    "    AttributeInfo(\n",
    "        name=\"register_num\",\n",
    "        description=\"Patent registration number\",\n",
    "        type=\"string\",\n",
    "    ),\n",
    "    AttributeInfo(\n",
    "        name=\"source\",\n",
    "        description=\"Source that published the document\",\n",
    "        type=\"string\",\n",
    "    ),\n",
    "    AttributeInfo(\n",
    "        name=\"title\",\n",
    "        description=\"Document title\",\n",
    "        type=\"string\",\n",
    "    ),\n",
    "    AttributeInfo(\n",
    "        name=\"type\",\n",
    "        description=\"Type of the document - possible value are article, lecture and patent\",\n",
    "        type=\"string\",\n",
    "    ),\n",
    "]\n",
    "\n",
    "document_content_description = \"Document content\"\n",
    "\n",
    "for name in names:\n",
    "    compressor = LLMChainExtractor.from_llm(llm)\n",
    "    all_retrievers[name]['self_query_retriever'] = SelfQueryRetriever.from_llm(llm = llm, \n",
    "                                                                               vectorstore = all_bases[name],\n",
    "                                                                               document_contents = document_content_description,\n",
    "                                                                               metadata_field_info = metadata_field_info, \n",
    "                                                                               verbose=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 116,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # ParentDocumentRetriever\n",
    "\n",
    "# from langchain.retrievers import ParentDocumentRetriever\n",
    "# from langchain.storage import InMemoryStore\n",
    "\n",
    "# vectorstore = Chroma(collection_name=\"split_children\", embedding_function=OpenAIEmbeddings())\n",
    "# store = Chroma(collection_name=\"split_parents\", embedding_function=OpenAIEmbeddings())\n",
    "\n",
    "# all_retrievers['recursive']['parent_retrieverfdfd'] = ParentDocumentRetriever(\n",
    "#     vectorstore = vectorstore, \n",
    "#     docstore = store, \n",
    "#     child_splitter = child_text_splitter,\n",
    "#     parent_splitter = text_splitter\n",
    "# )\n",
    "\n",
    "\n",
    "\n",
    "# # all_retrievers['recursive']['parent_retriever'].add_documents(documents = docs_single, ids = None)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 117,
   "metadata": {},
   "outputs": [],
   "source": [
    "# EnsembleRetriever\n",
    "\n",
    "from langchain.retrievers import EnsembleRetriever\n",
    "\n",
    "for name in names:\n",
    "    all_retrievers[name]['ensemble_retriever'] = EnsembleRetriever(retrievers = [r for r in all_retrievers[name].values()])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 118,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Prompt template\n",
    "\n",
    "from langchain import PromptTemplate\n",
    "\n",
    "prompt_template = PromptTemplate.from_template(\n",
    "    \"\"\"Use the following pieces of context to answer the question at the end.\n",
    "    If you don't know the answer, just say that you don't know, don't try to\n",
    "    make up an answer.\n",
    "\n",
    "    Context:\n",
    "    {context}\n",
    "\n",
    "    Question: {question}\n",
    "    Helpful Answer:\"\"\"\n",
    ")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 126,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Chains definition\n",
    "\n",
    "from langchain.chains import RetrievalQA\n",
    "\n",
    "all_chains = {}\n",
    "\n",
    "for name in names:\n",
    "    all_chains[name] = {}\n",
    "    for retriever_name, retriever in all_retrievers[name].items():\n",
    "        all_chains[name][retriever_name] = RetrievalQA.from_llm(llm = llm,\n",
    "                                                                prompt = prompt_template,\n",
    "                                                                retriever=retriever)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Test\n",
    "\n",
    "import datetime\n",
    "\n",
    "file_name = f\"responses_{str(datetime.datetime.now()).replace(' ', '_').replace(':', '_')}.txt\"\n",
    "queries = [\"What did Nikola Tesla think about Mars?\",\n",
    "           \"How many patents did Nikola Tesla file between 1890. and 1895.?\",\n",
    "           \"How did Nikola Tesla envision the future of travel?\",\n",
    "           \"What is Tesla's description of the human eye?\",\n",
    "           \"Where was Nikola Tesla born?\"]\n",
    "\n",
    "with open(file_name, 'w') as file:\n",
    "    file.write(f\"{str(datetime.datetime.now())}\\n\\n\")\n",
    "\n",
    "for q in queries:\n",
    "    with open(file_name, 'a') as file:\n",
    "        file.write(f\"- Query: {q}\\n\")\n",
    "        for name in names:\n",
    "            file.write(f\"   - Split: {name}\\n\")\n",
    "            \n",
    "            for k, chain in all_chains[name].items():\n",
    "                file.write(f\"      - Retriever: {k}\\n\")\n",
    "                try:\n",
    "                    file.write(f\"         - Response: {chain.run(q)}\\n\")\n",
    "                except Exception as e:\n",
    "                    file.write(f\"         - ERROR: {e}\\n\")\n",
    "        file.write(f\"\\n\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 204,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Results parser\n",
    "\n",
    "results = []\n",
    "end = False\n",
    "\n",
    "with open('responses_branimir.txt') as read_file:\n",
    "    while True:\n",
    "        for name in names:\n",
    "            for chain_name, _ in all_chains[name].items():\n",
    "                flag = False\n",
    "                result = None\n",
    "                for res in results:\n",
    "                    if name in res['split_name'] and chain_name in res['retriever']:\n",
    "                        flag = True\n",
    "                        result = res\n",
    "                if not flag:\n",
    "                    result = {'grades': [], 'split_name': name,'retriever': chain_name}\n",
    "                \n",
    "                while True:\n",
    "                    line = read_file.readline()\n",
    "                    if not line:\n",
    "                        end = True\n",
    "                        break\n",
    "                    if 'Response:' in line:\n",
    "                        result['grades'].append(int(line.split()[0]))\n",
    "                        break\n",
    "                    if 'ERROR' in line:\n",
    "                        result['grades'].append(1)\n",
    "                        break\n",
    "                if not flag:\n",
    "                    results.append(result)\n",
    "        if end:\n",
    "            break\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 205,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Helper functions\n",
    "\n",
    "def res_for_split(s):\n",
    "    ret = []\n",
    "    for res in results:\n",
    "        if res['split_name'] == s:\n",
    "            ret.append(res)\n",
    "    return ret\n",
    "\n",
    "def res_for_retriever(r):\n",
    "    ret = []\n",
    "    for res in results:\n",
    "        if res['retriever'] == r:\n",
    "            ret.append(res)\n",
    "    return ret\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 230,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "elements: 3.88\n",
      "nltk: 5.16\n",
      "recursive: 4.68\n"
     ]
    }
   ],
   "source": [
    "for name in names:\n",
    "    s = sum([sum(r['grades']) for r in res_for_split(name)])/25\n",
    "    print(f\"{name}: {s}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 225,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "3.12"
      ]
     },
     "execution_count": 225,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sum([sum(r['grades']) for r in res_for_retriever('self_query_retriever')])/15"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 236,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1"
      ]
     },
     "execution_count": 236,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "res_for_retriever('self_query_retriever')[0]['grades'].count(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 229,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "elements split with base - grades - [1, 1, 2, 1, 9] - average grade: 2.8\n",
      "elements split with multi_query - grades - [3, 6, 3, 1, 9] - average grade: 4.4\n",
      "elements split with compression_retriever - grades - [9, 3, 2, 1, 9] - average grade: 4.8\n",
      "elements split with self_query_retriever - grades - [1, 10, 1, 1, 7] - average grade: 4.0\n",
      "elements split with ensemble_retriever - grades - [3, 1, 3, 1, 9] - average grade: 3.4\n",
      "nltk split with base - grades - [4, 3, 5, 10, 6] - average grade: 5.6\n",
      "nltk split with multi_query - grades - [5, 1, 7, 8, 9] - average grade: 6.0\n",
      "nltk split with compression_retriever - grades - [2, 2, 8, 8, 6] - average grade: 5.2\n",
      "nltk split with self_query_retriever - grades - [1, 10, 9, 1, 7] - average grade: 5.6\n",
      "nltk split with ensemble_retriever - grades - [1, 1, 1, 8, 6] - average grade: 3.4\n",
      "recursive split with base - grades - [4, 1, 6, 9, 1] - average grade: 4.2\n",
      "recursive split with multi_query - grades - [4, 1, 10, 2, 9] - average grade: 5.2\n",
      "recursive split with compression_retriever - grades - [10, 1, 6, 7, 1] - average grade: 5.0\n",
      "recursive split with self_query_retriever - grades - [4, 10, 8, 1, 7] - average grade: 6.0\n",
      "recursive split with ensemble_retriever - grades - [4, 1, 6, 2, 2] - average grade: 3.0\n"
     ]
    }
   ],
   "source": [
    "for res in results:\n",
    "    l = res['grades']\n",
    "    mean = sum(l) / len(l)\n",
    "    print(f\"{res['split_name']} split with {res['retriever']} - grades - {res['grades']} - average grade: {mean}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.9"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
