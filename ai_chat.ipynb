{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 68,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Elements loader\n",
    "\n",
    "from langchain.document_loaders import UnstructuredWordDocumentLoader\n",
    "from langchain.document_loaders import DirectoryLoader\n",
    "\n",
    "elements_loader = DirectoryLoader(path = \"docx_docs\", \n",
    "                                  loader_cls = UnstructuredWordDocumentLoader,\n",
    "                                  loader_kwargs = {'mode' : \"elements\", 'strategy': \"fast\"},\n",
    "                                  recursive = True)\n",
    "\n",
    "docs_elements = elements_loader.load()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Recursive loaders\n",
    "\n",
    "from langchain.text_splitter import RecursiveCharacterTextSplitter\n",
    "from langchain.document_loaders import UnstructuredWordDocumentLoader\n",
    "from langchain.document_loaders import DirectoryLoader\n",
    "\n",
    "single_loader = DirectoryLoader(path = \"docx_docs\", \n",
    "                                loader_cls = UnstructuredWordDocumentLoader,\n",
    "                                loader_kwargs = {'mode' : \"single\", 'strategy': \"fast\"},\n",
    "                                recursive = True)\n",
    "docs_single = single_loader.load()\n",
    "\n",
    "text_splitter = RecursiveCharacterTextSplitter(chunk_size = 2000, chunk_overlap  = 0, separators = [\"\\n\\n\", \"(?<=\\n)\", \"(?<=\\. )\"], is_separator_regex = True)\n",
    "child_text_splitter = RecursiveCharacterTextSplitter(chunk_size = 500, chunk_overlap  = 0, separators = [\"\\n\\n\", \"(?<=\\n)\", \"(?<=\\. )\"], is_separator_regex = True)\n",
    "\n",
    "docs_recursive = text_splitter.split_documents(docs_single)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "metadata": {},
   "outputs": [],
   "source": [
    "# NLTK loader\n",
    "from langchain.text_splitter import NLTKTextSplitter\n",
    "\n",
    "nltk_splitter = NLTKTextSplitter()\n",
    "\n",
    "docs_nltk = nltk_splitter.split_documents(docs_single)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Finename metadata rename \n",
    "\n",
    "for doc in docs_nltk:\n",
    "    doc.metadata['filename'] = doc.metadata['source'].split('/')[-1]\n",
    "    del doc.metadata['source']\n",
    "\n",
    "for doc in docs_recursive:\n",
    "    doc.metadata['filename'] = doc.metadata['source'].split('/')[-1]\n",
    "    del doc.metadata['source']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Remove undeeded metadata from Elements documents\n",
    "\n",
    "keys_to_delete = ['source', 'file_directory', 'last_modified', 'filetype', 'primary', 'text_as_html', 'emphasized_text_tags', 'emphasized_text_contents']\n",
    "\n",
    "for doc in docs_elements:\n",
    "    for key in keys_to_delete:\n",
    "        if key in doc.metadata.keys():\n",
    "            del doc.metadata[key]\n",
    "\n",
    "# Removing unneeded documents\n",
    "categories_to_remove = ['PageBreak', 'ListItem', 'Footer', 'Table', 'UncategorizedText', 'Header']\n",
    "docs_elements = [doc for doc in docs_elements if doc.metadata['category'] not in categories_to_remove]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plots\n",
    "\n",
    "# from matplotlib import pyplot as plt\n",
    "\n",
    "# plt.figure(1)\n",
    "# plt.hist([len(d.page_content) for d in docs_nltk], bins = 100)\n",
    "# plt.grid()\n",
    "# plt.title(\"NLTK документи\")\n",
    "# plt.xlabel(\"број карактера у документу\")\n",
    "# plt.ylabel(\"број докумената\")\n",
    "# plt.figure(2)\n",
    "# plt.hist([len(d.page_content) for d in docs_elements], bins = 100)\n",
    "# plt.grid()\n",
    "# plt.title(\"Elements documents\")\n",
    "# plt.xlabel(\"број карактера у документу\")\n",
    "# plt.ylabel(\"број докумената\")\n",
    "# plt.figure(3)\n",
    "# plt.hist([len(d.page_content) for d in docs_recursive], bins = 100)\n",
    "# plt.grid()\n",
    "# plt.title(\"Recursive documents\")\n",
    "# plt.xlabel(\"број карактера у документу\")\n",
    "# plt.ylabel(\"број докумената\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Metadata parsing\n",
    "\n",
    "import json\n",
    "\n",
    "metadata_dict = {}\n",
    "metadatas = json.load(open('metadata.json'))\n",
    "\n",
    "ascii_replace_dict = {'â€™': '’', 'â€œ': '“', 'â€': '”'}\n",
    "def replace_non_ascii(s):\n",
    "    for pair_k, pair_v in ascii_replace_dict.items():\n",
    "        s = s.replace(pair_k, pair_v)\n",
    "    return s\n",
    "\n",
    "for metadata in metadatas[2]['data']:\n",
    "    if metadata['id'] in ['228', '413']:\n",
    "        # These are duplicates, not needed\n",
    "        continue\n",
    "    key = ''.join(metadata['file_url'].split('/')[-1].rsplit('.', 1)[:-1])\n",
    "\n",
    "    # Repalce non-ascii characters in key\n",
    "    key = replace_non_ascii(key)\n",
    "    \n",
    "    # Make sure that there are no duplicate entries for the same document type\n",
    "    if (key in metadata_dict.keys() and metadata['type'] == metadata_dict[key]['type']):\n",
    "        print(key)\n",
    "        print(metadata['id'])\n",
    "    assert not (key in metadata_dict.keys() and metadata['type'] == metadata_dict[key]['type'])\n",
    "    metadata_dict[key] = {key: value for key, value in metadata.items() if value is not None}\n",
    "    \n",
    "    # Replace non-ascii characters in file url\n",
    "    del metadata_dict[key]['file_url']\n",
    "    for m_key, m_value in metadata_dict.items():\n",
    "        if m_value is None:\n",
    "            del metadata_dict[m_key]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Adding metadata\n",
    "\n",
    "for doc in docs_elements:\n",
    "    assert doc.metadata['filename'][:-5] in metadata_dict.keys()\n",
    "    doc.metadata.update(metadata_dict[doc.metadata['filename'][:-5]])\n",
    "\n",
    "for doc in docs_nltk:\n",
    "    assert doc.metadata['filename'][:-5] in metadata_dict.keys()\n",
    "    doc.metadata.update(metadata_dict[doc.metadata['filename'][:-5]])\n",
    "\n",
    "for doc in docs_recursive:\n",
    "    assert doc.metadata['filename'][:-5] in metadata_dict.keys()\n",
    "    doc.metadata.update(metadata_dict[doc.metadata['filename'][:-5]])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 121,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Summary document\n",
    "\n",
    "file_path = 'summary.txt'\n",
    "\n",
    "with open(file_path, 'w') as file:\n",
    "    file.write('This file is a summary document of all of the articles, lectures and patents\\n')\n",
    "    file.write('\\nArticles:\\n')\n",
    "    file.write('\\nLectures:\\n')\n",
    "    file.write('\\nPatents:\\n')\n",
    "\n",
    "a_num = 0\n",
    "l_num = 0\n",
    "p_num = 0\n",
    "\n",
    "for metadata_key in metadata_dict.keys():\n",
    "    with open(file_path, 'r') as file:\n",
    "        lines = file.readlines()\n",
    "\n",
    "    new_lines = []\n",
    "\n",
    "    a_num += 1 if metadata_dict[metadata_key]['type'] == 'article' else 0\n",
    "    l_num += 1 if metadata_dict[metadata_key]['type'] == 'lecture' else 0\n",
    "    p_num += 1 if metadata_dict[metadata_key]['type'] == 'patent' else 0\n",
    "\n",
    "    for i, line in enumerate(lines):\n",
    "        new_lines.append(line)\n",
    "        \n",
    "        if metadata_dict[metadata_key]['type'] == 'article':\n",
    "            if 'Articles:' in line:\n",
    "                new_line = f\"    - Article {metadata_dict[metadata_key]['title']} written in {metadata_dict[metadata_key]['date']} published by {metadata_dict[metadata_key]['source']}\\n\"\n",
    "                new_lines.append(new_line)\n",
    "        \n",
    "        if metadata_dict[metadata_key]['type'] == 'lecture':\n",
    "            if 'Lectures:' in line:\n",
    "                new_line = f\"    - Lecture {metadata_dict[metadata_key]['title']} held in {metadata_dict[metadata_key]['date']}\\n\"\n",
    "                new_lines.append(new_line)\n",
    "        \n",
    "        if metadata_dict[metadata_key]['type'] == 'patent':\n",
    "            if 'Patents:' in line:\n",
    "                new_line = f\"    - Patent {metadata_dict[metadata_key]['title']} filed in {metadata_dict[metadata_key]['date']} with registration number {metadata_dict[metadata_key]['register_num']}\\n\"\n",
    "                new_lines.append(new_line)\n",
    "\n",
    "\n",
    "    with open(file_path, 'w') as file:\n",
    "        file.writelines(new_lines)\n",
    "\n",
    "    \n",
    "with open(file_path, 'r') as file:\n",
    "    lines = file.readlines()\n",
    "new_lines = []\n",
    "for i, line in enumerate(lines):\n",
    "    if 'Patents:' in line:\n",
    "        new_lines.append(f\"Patents ({p_num} files):\\n\")\n",
    "    elif 'Articles:' in line:\n",
    "        new_lines.append(f\"Articles ({a_num} files):\\n\")\n",
    "    elif 'Lectures:' in line:\n",
    "        new_lines.append(f\"Lectures ({l_num} files):\\n\")\n",
    "    else:\n",
    "        new_lines.append(line)\n",
    "with open(file_path, 'w') as file:\n",
    "    file.writelines(new_lines)\n",
    "\n",
    "from langchain.document_loaders import TextLoader\n",
    "\n",
    "doc = TextLoader(file_path).load()\n",
    "\n",
    "docs_metadata = text_splitter.split_documents(doc)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Vectorstore creation\n",
    "\n",
    "from langchain.vectorstores import Chroma\n",
    "from langchain.embeddings import OpenAIEmbeddings\n",
    "\n",
    "embeddings = OpenAIEmbeddings()\n",
    "\n",
    "# vectorstore_elements = Chroma.from_documents(\n",
    "#     collection_name=\"elements\",\n",
    "#     documents = docs_elements + docs_metadata,\n",
    "#     embedding = embeddings,\n",
    "#     persist_directory=\"./vectorstore_elements\"\n",
    "# )\n",
    "\n",
    "# vectorstore_nltk = Chroma.from_documents(\n",
    "#     collection_name=\"nltk\",\n",
    "#     documents = docs_nltk + docs_metadata,\n",
    "#     embedding = embeddings,\n",
    "#     persist_directory=\"./vectorstore_nltk\"\n",
    "# )\n",
    "\n",
    "# vectorstore_recursive = Chroma.from_documents(\n",
    "#     collection_name=\"recursive\",\n",
    "#     documents = docs_recursive + docs_metadata,\n",
    "#     embedding = embeddings,\n",
    "#     persist_directory=\"./vectorstore_recursive\"\n",
    "# )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# LLM loading\n",
    "\n",
    "from langchain.chat_models import ChatOpenAI\n",
    "\n",
    "llm_name = \"gpt-3.5-turbo\"\n",
    "llm = ChatOpenAI(model_name = llm_name, temperature = 0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Vectorstore loading\n",
    "\n",
    "from langchain.vectorstores import Chroma\n",
    "from langchain.embeddings import OpenAIEmbeddings\n",
    "\n",
    "embeddings = OpenAIEmbeddings()\n",
    "\n",
    "names = ['elements', 'nltk', 'recursive']\n",
    "all_bases = {}\n",
    "all_retrievers = {}\n",
    "\n",
    "for name in names:\n",
    "    all_bases[name] = Chroma(persist_directory=f\"./vectorstore_{name}\", embedding_function = embeddings)\n",
    "    all_retrievers[name] = {}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# BaseRetriever\n",
    "\n",
    "for name in names:\n",
    "    all_retrievers[name]['base'] = all_bases[name].as_retriever(search_type = 'mmr')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# MultiQueryRetriever\n",
    "\n",
    "from langchain.retrievers.multi_query import MultiQueryRetriever\n",
    "\n",
    "for name in names:\n",
    "    all_retrievers[name]['multi_query'] = MultiQueryRetriever.from_llm(retriever = all_bases[name].as_retriever(search_type = 'mmr'), \n",
    "                                                                       llm = llm)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ContextualCompressionRetriever\n",
    "from langchain.retrievers import ContextualCompressionRetriever\n",
    "from langchain.retrievers.document_compressors import LLMChainExtractor\n",
    "\n",
    "for name in names:\n",
    "    compressor = LLMChainExtractor.from_llm(llm)\n",
    "    all_retrievers[name]['compression_retriever'] = ContextualCompressionRetriever(base_compressor = compressor, \n",
    "                                                                                   base_retriever = all_retrievers[name]['base'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'elements': {'base': VectorStoreRetriever(tags=['Chroma', 'OpenAIEmbeddings'], metadata=None, vectorstore=<langchain.vectorstores.chroma.Chroma object at 0x7f47f3b1fc10>, search_type='mmr', search_kwargs={}),\n",
       "  'multi_query': MultiQueryRetriever(tags=None, metadata=None, retriever=VectorStoreRetriever(tags=['Chroma', 'OpenAIEmbeddings'], metadata=None, vectorstore=<langchain.vectorstores.chroma.Chroma object at 0x7f47f3b1fc10>, search_type='mmr', search_kwargs={}), llm_chain=LLMChain(memory=None, callbacks=None, callback_manager=None, verbose=False, tags=None, metadata=None, prompt=PromptTemplate(input_variables=['question'], output_parser=None, partial_variables={}, template='You are an AI language model assistant. Your task is \\n    to generate 3 different versions of the given user \\n    question to retrieve relevant documents from a vector  database. \\n    By generating multiple perspectives on the user question, \\n    your goal is to help the user overcome some of the limitations \\n    of distance-based similarity search. Provide these alternative \\n    questions separated by newlines. Original question: {question}', template_format='f-string', validate_template=True), llm=ChatOpenAI(cache=None, verbose=False, callbacks=None, callback_manager=None, tags=None, metadata=None, client=<class 'openai.api_resources.chat_completion.ChatCompletion'>, model_name='gpt-3.5-turbo', temperature=0.0, model_kwargs={}, openai_api_key='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', openai_api_base='', openai_organization='', openai_proxy='', request_timeout=None, max_retries=6, streaming=False, n=1, max_tokens=None, tiktoken_model_name=None), output_key='text', output_parser=LineListOutputParser(pydantic_object=<class 'langchain.retrievers.multi_query.LineList'>), return_final_only=True, llm_kwargs={}), verbose=True, parser_key='lines'),\n",
       "  'compression_retriever': ContextualCompressionRetriever(tags=None, metadata=None, base_compressor=LLMChainExtractor(llm_chain=LLMChain(memory=None, callbacks=None, callback_manager=None, verbose=False, tags=None, metadata=None, prompt=PromptTemplate(input_variables=['question', 'context'], output_parser=NoOutputParser(no_output_str='NO_OUTPUT'), partial_variables={}, template='Given the following question and context, extract any part of the context *AS IS* that is relevant to answer the question. If none of the context is relevant return NO_OUTPUT. \\n\\nRemember, *DO NOT* edit the extracted parts of the context.\\n\\n> Question: {question}\\n> Context:\\n>>>\\n{context}\\n>>>\\nExtracted relevant parts:', template_format='f-string', validate_template=True), llm=ChatOpenAI(cache=None, verbose=False, callbacks=None, callback_manager=None, tags=None, metadata=None, client=<class 'openai.api_resources.chat_completion.ChatCompletion'>, model_name='gpt-3.5-turbo', temperature=0.0, model_kwargs={}, openai_api_key='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', openai_api_base='', openai_organization='', openai_proxy='', request_timeout=None, max_retries=6, streaming=False, n=1, max_tokens=None, tiktoken_model_name=None), output_key='text', output_parser=StrOutputParser(), return_final_only=True, llm_kwargs={}), get_input=<function default_get_input at 0x7f47f09f2a70>), base_retriever=VectorStoreRetriever(tags=['Chroma', 'OpenAIEmbeddings'], metadata=None, vectorstore=<langchain.vectorstores.chroma.Chroma object at 0x7f47f3b1fc10>, search_type='mmr', search_kwargs={}))},\n",
       " 'nltk': {'base': VectorStoreRetriever(tags=['Chroma', 'OpenAIEmbeddings'], metadata=None, vectorstore=<langchain.vectorstores.chroma.Chroma object at 0x7f47f0d114e0>, search_type='mmr', search_kwargs={}),\n",
       "  'multi_query': MultiQueryRetriever(tags=None, metadata=None, retriever=VectorStoreRetriever(tags=['Chroma', 'OpenAIEmbeddings'], metadata=None, vectorstore=<langchain.vectorstores.chroma.Chroma object at 0x7f47f0d114e0>, search_type='mmr', search_kwargs={}), llm_chain=LLMChain(memory=None, callbacks=None, callback_manager=None, verbose=False, tags=None, metadata=None, prompt=PromptTemplate(input_variables=['question'], output_parser=None, partial_variables={}, template='You are an AI language model assistant. Your task is \\n    to generate 3 different versions of the given user \\n    question to retrieve relevant documents from a vector  database. \\n    By generating multiple perspectives on the user question, \\n    your goal is to help the user overcome some of the limitations \\n    of distance-based similarity search. Provide these alternative \\n    questions separated by newlines. Original question: {question}', template_format='f-string', validate_template=True), llm=ChatOpenAI(cache=None, verbose=False, callbacks=None, callback_manager=None, tags=None, metadata=None, client=<class 'openai.api_resources.chat_completion.ChatCompletion'>, model_name='gpt-3.5-turbo', temperature=0.0, model_kwargs={}, openai_api_key='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', openai_api_base='', openai_organization='', openai_proxy='', request_timeout=None, max_retries=6, streaming=False, n=1, max_tokens=None, tiktoken_model_name=None), output_key='text', output_parser=LineListOutputParser(pydantic_object=<class 'langchain.retrievers.multi_query.LineList'>), return_final_only=True, llm_kwargs={}), verbose=True, parser_key='lines'),\n",
       "  'compression_retriever': ContextualCompressionRetriever(tags=None, metadata=None, base_compressor=LLMChainExtractor(llm_chain=LLMChain(memory=None, callbacks=None, callback_manager=None, verbose=False, tags=None, metadata=None, prompt=PromptTemplate(input_variables=['question', 'context'], output_parser=NoOutputParser(no_output_str='NO_OUTPUT'), partial_variables={}, template='Given the following question and context, extract any part of the context *AS IS* that is relevant to answer the question. If none of the context is relevant return NO_OUTPUT. \\n\\nRemember, *DO NOT* edit the extracted parts of the context.\\n\\n> Question: {question}\\n> Context:\\n>>>\\n{context}\\n>>>\\nExtracted relevant parts:', template_format='f-string', validate_template=True), llm=ChatOpenAI(cache=None, verbose=False, callbacks=None, callback_manager=None, tags=None, metadata=None, client=<class 'openai.api_resources.chat_completion.ChatCompletion'>, model_name='gpt-3.5-turbo', temperature=0.0, model_kwargs={}, openai_api_key='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', openai_api_base='', openai_organization='', openai_proxy='', request_timeout=None, max_retries=6, streaming=False, n=1, max_tokens=None, tiktoken_model_name=None), output_key='text', output_parser=StrOutputParser(), return_final_only=True, llm_kwargs={}), get_input=<function default_get_input at 0x7f47f09f2a70>), base_retriever=VectorStoreRetriever(tags=['Chroma', 'OpenAIEmbeddings'], metadata=None, vectorstore=<langchain.vectorstores.chroma.Chroma object at 0x7f47f0d114e0>, search_type='mmr', search_kwargs={}))},\n",
       " 'recursive': {'base': VectorStoreRetriever(tags=['Chroma', 'OpenAIEmbeddings'], metadata=None, vectorstore=<langchain.vectorstores.chroma.Chroma object at 0x7f47f0d99a20>, search_type='mmr', search_kwargs={}),\n",
       "  'multi_query': MultiQueryRetriever(tags=None, metadata=None, retriever=VectorStoreRetriever(tags=['Chroma', 'OpenAIEmbeddings'], metadata=None, vectorstore=<langchain.vectorstores.chroma.Chroma object at 0x7f47f0d99a20>, search_type='mmr', search_kwargs={}), llm_chain=LLMChain(memory=None, callbacks=None, callback_manager=None, verbose=False, tags=None, metadata=None, prompt=PromptTemplate(input_variables=['question'], output_parser=None, partial_variables={}, template='You are an AI language model assistant. Your task is \\n    to generate 3 different versions of the given user \\n    question to retrieve relevant documents from a vector  database. \\n    By generating multiple perspectives on the user question, \\n    your goal is to help the user overcome some of the limitations \\n    of distance-based similarity search. Provide these alternative \\n    questions separated by newlines. Original question: {question}', template_format='f-string', validate_template=True), llm=ChatOpenAI(cache=None, verbose=False, callbacks=None, callback_manager=None, tags=None, metadata=None, client=<class 'openai.api_resources.chat_completion.ChatCompletion'>, model_name='gpt-3.5-turbo', temperature=0.0, model_kwargs={}, openai_api_key='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', openai_api_base='', openai_organization='', openai_proxy='', request_timeout=None, max_retries=6, streaming=False, n=1, max_tokens=None, tiktoken_model_name=None), output_key='text', output_parser=LineListOutputParser(pydantic_object=<class 'langchain.retrievers.multi_query.LineList'>), return_final_only=True, llm_kwargs={}), verbose=True, parser_key='lines'),\n",
       "  'compression_retriever': ContextualCompressionRetriever(tags=None, metadata=None, base_compressor=LLMChainExtractor(llm_chain=LLMChain(memory=None, callbacks=None, callback_manager=None, verbose=False, tags=None, metadata=None, prompt=PromptTemplate(input_variables=['question', 'context'], output_parser=NoOutputParser(no_output_str='NO_OUTPUT'), partial_variables={}, template='Given the following question and context, extract any part of the context *AS IS* that is relevant to answer the question. If none of the context is relevant return NO_OUTPUT. \\n\\nRemember, *DO NOT* edit the extracted parts of the context.\\n\\n> Question: {question}\\n> Context:\\n>>>\\n{context}\\n>>>\\nExtracted relevant parts:', template_format='f-string', validate_template=True), llm=ChatOpenAI(cache=None, verbose=False, callbacks=None, callback_manager=None, tags=None, metadata=None, client=<class 'openai.api_resources.chat_completion.ChatCompletion'>, model_name='gpt-3.5-turbo', temperature=0.0, model_kwargs={}, openai_api_key='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', openai_api_base='', openai_organization='', openai_proxy='', request_timeout=None, max_retries=6, streaming=False, n=1, max_tokens=None, tiktoken_model_name=None), output_key='text', output_parser=StrOutputParser(), return_final_only=True, llm_kwargs={}), get_input=<function default_get_input at 0x7f47f09f2a70>), base_retriever=VectorStoreRetriever(tags=['Chroma', 'OpenAIEmbeddings'], metadata=None, vectorstore=<langchain.vectorstores.chroma.Chroma object at 0x7f47f0d99a20>, search_type='mmr', search_kwargs={}))}}"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "all_retrievers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# SelfQueryRetriever\n",
    "\n",
    "from langchain.chains.query_constructor.base import AttributeInfo\n",
    "from langchain.retrievers.self_query.base import SelfQueryRetriever\n",
    "\n",
    "metadata_field_info = [\n",
    "    AttributeInfo(\n",
    "        name=\"category\",\n",
    "        description=\"Category of te text content - possible values are NarrativeText and Title\",\n",
    "        type=\"string\",\n",
    "    ),\n",
    "    AttributeInfo(\n",
    "        name=\"date\",\n",
    "        description=\"Date when the document was published\",\n",
    "        type=\"string\",\n",
    "    ),\n",
    "    AttributeInfo(\n",
    "        name=\"filename\",\n",
    "        description=\"Name of the file\",\n",
    "        type=\"string\",\n",
    "    ),\n",
    "    AttributeInfo(\n",
    "        name=\"id\",\n",
    "        description=\"Document ID\",\n",
    "        type=\"string\",\n",
    "    ),\n",
    "    AttributeInfo(\n",
    "        name=\"page_number\",\n",
    "        description=\"Page number from the original document\",\n",
    "        type=\"string\",\n",
    "    ),\n",
    "    AttributeInfo(\n",
    "        name=\"register_num\",\n",
    "        description=\"Patent registration number\",\n",
    "        type=\"string\",\n",
    "    ),\n",
    "    AttributeInfo(\n",
    "        name=\"source\",\n",
    "        description=\"Source that published the document\",\n",
    "        type=\"string\",\n",
    "    ),\n",
    "    AttributeInfo(\n",
    "        name=\"title\",\n",
    "        description=\"Document title\",\n",
    "        type=\"string\",\n",
    "    ),\n",
    "    AttributeInfo(\n",
    "        name=\"type\",\n",
    "        description=\"Type of the document - possible value are article, lecture and patent\",\n",
    "        type=\"string\",\n",
    "    ),\n",
    "]\n",
    "\n",
    "document_content_description = \"Document content\"\n",
    "\n",
    "self_query_retriever_elements = SelfQueryRetriever.from_llm(\n",
    "    llm = llm, \n",
    "    vectorstore = vectorstore_elements,\n",
    "    document_contents = document_content_description,\n",
    "    metadata_field_info = metadata_field_info, \n",
    "    verbose=True\n",
    ")\n",
    "\n",
    "self_query_retriever_nltk = SelfQueryRetriever.from_llm(\n",
    "    llm = llm, \n",
    "    vectorstore = vectorstore_nltk,\n",
    "    document_contents = document_content_description,\n",
    "    metadata_field_info = metadata_field_info, \n",
    "    verbose=True\n",
    ")\n",
    "\n",
    "self_query_retriever_recursive = SelfQueryRetriever.from_llm(\n",
    "    llm = llm, \n",
    "    vectorstore = vectorstore_recursive,\n",
    "    document_contents = document_content_description,\n",
    "    metadata_field_info = metadata_field_info, \n",
    "    verbose=True\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/jaksa/miniconda3/lib/python3.10/site-packages/langchain/chains/llm.py:278: UserWarning: The predict_and_parse method is deprecated, instead pass an output parser directly to LLMChain.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "query='Nikola Tesla' filter=Comparison(comparator=<Comparator.EQ: 'eq'>, attribute='type', value='lecture') limit=None\n"
     ]
    }
   ],
   "source": [
    "docs = retriever.get_relevant_documents(\"What lectures did Nikola Tesla give\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['On Light and Other High Frequency Phenomena.docx',\n",
       " 'Experiments With Alternate Currents of High Potential and High Frequency (lecture).docx',\n",
       " 'A New System of Alternate Current Motors and Transformers (lecture).docx',\n",
       " '8 High Frequency Oscillators for Electro-Therapeutic and Other Purposes (lecture).docx']"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "[d.metadata['filename'] for d in docs]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "query='electromotor poles' filter=None limit=None\n",
      "{'query': 'How many poles shoul my electromotor have, and what should I do if I have the wrong number?', 'result': 'Based on the given context, the number of poles in an electromotor can vary. The motor X has eight poles, the motor Y has six poles, and the motor Z has four poles. The number of poles determines the speed of the motor. If you have the wrong number of poles, you can change the electrical connections to achieve the desired speed. For example, in motor X, you can alternate between two like and two opposite poles to effectively reduce the number of poles by half and double the speed of the motor.'}\n"
     ]
    }
   ],
   "source": [
    "# Basic Retriever example\n",
    "\n",
    "from langchain.chains import RetrievalQA\n",
    "\n",
    "question = \"How many poles shoul my electromotor have, and what should I do if I have the wrong number?\"\n",
    "qa_chain = RetrievalQA.from_chain_type(llm = llm, retriever = retriever)\n",
    "print(qa_chain({\"query\": question}))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Retriever with memory example\n",
    "\n",
    "from langchain.memory import ConversationBufferMemory\n",
    "from langchain.chains import ConversationalRetrievalChain\n",
    "\n",
    "memory = ConversationBufferMemory(memory_key = \"chat_history\", return_messages = True)\n",
    "retriever = vectorstore.as_retriever()\n",
    "qa = ConversationalRetrievalChain.from_llm(llm = llm, retriever = retriever, memory = memory)\n",
    "question = input()\n",
    "print(f\"Question: {question}\")\n",
    "print(f\"Answer: {qa({'question': question})['answer']}\")\n",
    "question = input()\n",
    "print(f\"Question: {question}\")\n",
    "print(f\"Answer: {qa({'question': question})['answer']}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "q = \"\"\"Use the following pieces of context to answer the question at the end.\n",
    "If you don't know the answer, just say that you don't know, don't try to\n",
    "make up an answer.\n",
    "\n",
    "{context} // i.e the pdf text content\n",
    "\n",
    "Question: {query} // i.e our actualy query, 'Who is the CV about?'\n",
    "Helpful Answer:\"\"\""
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.9"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
