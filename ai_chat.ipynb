{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 68,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Elements loader\n",
    "\n",
    "from langchain.document_loaders import UnstructuredWordDocumentLoader\n",
    "from langchain.document_loaders import DirectoryLoader\n",
    "\n",
    "elements_loader = DirectoryLoader(path = \"docx_docs\", \n",
    "                                  loader_cls = UnstructuredWordDocumentLoader,\n",
    "                                  loader_kwargs = {'mode' : \"elements\", 'strategy': \"fast\"},\n",
    "                                  recursive = True)\n",
    "\n",
    "docs_elements = elements_loader.load()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Recursive loaders\n",
    "\n",
    "from langchain.text_splitter import RecursiveCharacterTextSplitter\n",
    "from langchain.document_loaders import UnstructuredWordDocumentLoader\n",
    "from langchain.document_loaders import DirectoryLoader\n",
    "\n",
    "single_loader = DirectoryLoader(path = \"docx_docs\", \n",
    "                                loader_cls = UnstructuredWordDocumentLoader,\n",
    "                                loader_kwargs = {'mode' : \"single\", 'strategy': \"fast\"},\n",
    "                                recursive = True)\n",
    "docs_single = single_loader.load()\n",
    "\n",
    "text_splitter = RecursiveCharacterTextSplitter(chunk_size = 2000, chunk_overlap  = 0, separators = [\"\\n\\n\", \"(?<=\\n)\", \"(?<=\\. )\"], is_separator_regex = True)\n",
    "child_text_splitter = RecursiveCharacterTextSplitter(chunk_size = 500, chunk_overlap  = 0, separators = [\"\\n\\n\", \"(?<=\\n)\", \"(?<=\\. )\"], is_separator_regex = True)\n",
    "\n",
    "docs_recursive = text_splitter.split_documents(docs_single)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "metadata": {},
   "outputs": [],
   "source": [
    "# NLTK loader\n",
    "from langchain.text_splitter import NLTKTextSplitter\n",
    "\n",
    "nltk_splitter = NLTKTextSplitter()\n",
    "\n",
    "docs_nltk = nltk_splitter.split_documents(docs_single)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Finename metadata rename \n",
    "\n",
    "for doc in docs_nltk:\n",
    "    doc.metadata['filename'] = doc.metadata['source'].split('/')[-1]\n",
    "    del doc.metadata['source']\n",
    "\n",
    "for doc in docs_recursive:\n",
    "    doc.metadata['filename'] = doc.metadata['source'].split('/')[-1]\n",
    "    del doc.metadata['source']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Remove undeeded metadata from Elements documents\n",
    "\n",
    "keys_to_delete = ['source', 'file_directory', 'last_modified', 'filetype', 'primary', 'text_as_html', 'emphasized_text_tags', 'emphasized_text_contents']\n",
    "\n",
    "for doc in docs_elements:\n",
    "    for key in keys_to_delete:\n",
    "        if key in doc.metadata.keys():\n",
    "            del doc.metadata[key]\n",
    "\n",
    "# Removing unneeded documents\n",
    "categories_to_remove = ['PageBreak', 'ListItem', 'Footer', 'Table', 'UncategorizedText', 'Header']\n",
    "docs_elements = [doc for doc in docs_elements if doc.metadata['category'] not in categories_to_remove]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plots\n",
    "\n",
    "# from matplotlib import pyplot as plt\n",
    "\n",
    "# plt.figure(1)\n",
    "# plt.hist([len(d.page_content) for d in docs_nltk], bins = 100)\n",
    "# plt.grid()\n",
    "# plt.title(\"NLTK документи\")\n",
    "# plt.xlabel(\"број карактера у документу\")\n",
    "# plt.ylabel(\"број докумената\")\n",
    "# plt.figure(2)\n",
    "# plt.hist([len(d.page_content) for d in docs_elements], bins = 100)\n",
    "# plt.grid()\n",
    "# plt.title(\"Elements documents\")\n",
    "# plt.xlabel(\"број карактера у документу\")\n",
    "# plt.ylabel(\"број докумената\")\n",
    "# plt.figure(3)\n",
    "# plt.hist([len(d.page_content) for d in docs_recursive], bins = 100)\n",
    "# plt.grid()\n",
    "# plt.title(\"Recursive documents\")\n",
    "# plt.xlabel(\"број карактера у документу\")\n",
    "# plt.ylabel(\"број докумената\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Metadata parsing\n",
    "\n",
    "import json\n",
    "\n",
    "metadata_dict = {}\n",
    "metadatas = json.load(open('metadata.json'))\n",
    "\n",
    "ascii_replace_dict = {'â€™': '’', 'â€œ': '“', 'â€': '”'}\n",
    "def replace_non_ascii(s):\n",
    "    for pair_k, pair_v in ascii_replace_dict.items():\n",
    "        s = s.replace(pair_k, pair_v)\n",
    "    return s\n",
    "\n",
    "for metadata in metadatas[2]['data']:\n",
    "    if metadata['id'] in ['228', '413']:\n",
    "        # These are duplicates, not needed\n",
    "        continue\n",
    "    key = ''.join(metadata['file_url'].split('/')[-1].rsplit('.', 1)[:-1])\n",
    "\n",
    "    # Repalce non-ascii characters in key\n",
    "    key = replace_non_ascii(key)\n",
    "    \n",
    "    # Make sure that there are no duplicate entries for the same document type\n",
    "    if (key in metadata_dict.keys() and metadata['type'] == metadata_dict[key]['type']):\n",
    "        print(key)\n",
    "        print(metadata['id'])\n",
    "    assert not (key in metadata_dict.keys() and metadata['type'] == metadata_dict[key]['type'])\n",
    "    metadata_dict[key] = {key: value for key, value in metadata.items() if value is not None}\n",
    "    \n",
    "    # Replace non-ascii characters in file url\n",
    "    del metadata_dict[key]['file_url']\n",
    "    for m_key, m_value in metadata_dict.items():\n",
    "        if m_value is None:\n",
    "            del metadata_dict[m_key]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Adding metadata\n",
    "\n",
    "for doc in docs_elements:\n",
    "    assert doc.metadata['filename'][:-5] in metadata_dict.keys()\n",
    "    doc.metadata.update(metadata_dict[doc.metadata['filename'][:-5]])\n",
    "\n",
    "for doc in docs_nltk:\n",
    "    assert doc.metadata['filename'][:-5] in metadata_dict.keys()\n",
    "    doc.metadata.update(metadata_dict[doc.metadata['filename'][:-5]])\n",
    "\n",
    "for doc in docs_recursive:\n",
    "    assert doc.metadata['filename'][:-5] in metadata_dict.keys()\n",
    "    doc.metadata.update(metadata_dict[doc.metadata['filename'][:-5]])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 121,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Summary document\n",
    "\n",
    "file_path = 'summary.txt'\n",
    "\n",
    "with open(file_path, 'w') as file:\n",
    "    file.write('This file is a summary document of all of the articles, lectures and patents\\n')\n",
    "    file.write('\\nArticles:\\n')\n",
    "    file.write('\\nLectures:\\n')\n",
    "    file.write('\\nPatents:\\n')\n",
    "\n",
    "a_num = 0\n",
    "l_num = 0\n",
    "p_num = 0\n",
    "\n",
    "for metadata_key in metadata_dict.keys():\n",
    "    with open(file_path, 'r') as file:\n",
    "        lines = file.readlines()\n",
    "\n",
    "    new_lines = []\n",
    "\n",
    "    a_num += 1 if metadata_dict[metadata_key]['type'] == 'article' else 0\n",
    "    l_num += 1 if metadata_dict[metadata_key]['type'] == 'lecture' else 0\n",
    "    p_num += 1 if metadata_dict[metadata_key]['type'] == 'patent' else 0\n",
    "\n",
    "    for i, line in enumerate(lines):\n",
    "        new_lines.append(line)\n",
    "        \n",
    "        if metadata_dict[metadata_key]['type'] == 'article':\n",
    "            if 'Articles:' in line:\n",
    "                new_line = f\"    - Article {metadata_dict[metadata_key]['title']} written in {metadata_dict[metadata_key]['date']} published by {metadata_dict[metadata_key]['source']}\\n\"\n",
    "                new_lines.append(new_line)\n",
    "        \n",
    "        if metadata_dict[metadata_key]['type'] == 'lecture':\n",
    "            if 'Lectures:' in line:\n",
    "                new_line = f\"    - Lecture {metadata_dict[metadata_key]['title']} held in {metadata_dict[metadata_key]['date']}\\n\"\n",
    "                new_lines.append(new_line)\n",
    "        \n",
    "        if metadata_dict[metadata_key]['type'] == 'patent':\n",
    "            if 'Patents:' in line:\n",
    "                new_line = f\"    - Patent {metadata_dict[metadata_key]['title']} filed in {metadata_dict[metadata_key]['date']} with registration number {metadata_dict[metadata_key]['register_num']}\\n\"\n",
    "                new_lines.append(new_line)\n",
    "\n",
    "\n",
    "    with open(file_path, 'w') as file:\n",
    "        file.writelines(new_lines)\n",
    "\n",
    "    \n",
    "with open(file_path, 'r') as file:\n",
    "    lines = file.readlines()\n",
    "new_lines = []\n",
    "for i, line in enumerate(lines):\n",
    "    if 'Patents:' in line:\n",
    "        new_lines.append(f\"Patents ({p_num} files):\\n\")\n",
    "    elif 'Articles:' in line:\n",
    "        new_lines.append(f\"Articles ({a_num} files):\\n\")\n",
    "    elif 'Lectures:' in line:\n",
    "        new_lines.append(f\"Lectures ({l_num} files):\\n\")\n",
    "    else:\n",
    "        new_lines.append(line)\n",
    "with open(file_path, 'w') as file:\n",
    "    file.writelines(new_lines)\n",
    "\n",
    "from langchain.document_loaders import TextLoader\n",
    "\n",
    "doc = TextLoader(file_path).load()\n",
    "\n",
    "docs_metadata = text_splitter.split_documents(doc)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 138,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Retrying langchain.embeddings.openai.embed_with_retry.<locals>._embed_with_retry in 4.0 seconds as it raised RateLimitError: Rate limit reached for default-text-embedding-ada-002 in organization org-RyMWzYr9lGLc1ON8lKEsa30u on tokens per min. Limit: 1000000 / min. Current: 725591 / min. Contact us through our help center at help.openai.com if you continue to have issues..\n",
      "Retrying langchain.embeddings.openai.embed_with_retry.<locals>._embed_with_retry in 4.0 seconds as it raised RateLimitError: Rate limit reached for default-text-embedding-ada-002 in organization org-RyMWzYr9lGLc1ON8lKEsa30u on tokens per min. Limit: 1000000 / min. Current: 771656 / min. Contact us through our help center at help.openai.com if you continue to have issues..\n",
      "Retrying langchain.embeddings.openai.embed_with_retry.<locals>._embed_with_retry in 4.0 seconds as it raised RateLimitError: Rate limit reached for default-text-embedding-ada-002 in organization org-RyMWzYr9lGLc1ON8lKEsa30u on tokens per min. Limit: 1000000 / min. Current: 833359 / min. Contact us through our help center at help.openai.com if you continue to have issues..\n",
      "Retrying langchain.embeddings.openai.embed_with_retry.<locals>._embed_with_retry in 4.0 seconds as it raised RateLimitError: Rate limit reached for default-text-embedding-ada-002 in organization org-RyMWzYr9lGLc1ON8lKEsa30u on tokens per min. Limit: 1000000 / min. Current: 743673 / min. Contact us through our help center at help.openai.com if you continue to have issues..\n",
      "Retrying langchain.embeddings.openai.embed_with_retry.<locals>._embed_with_retry in 4.0 seconds as it raised RateLimitError: Rate limit reached for default-text-embedding-ada-002 in organization org-RyMWzYr9lGLc1ON8lKEsa30u on tokens per min. Limit: 1000000 / min. Current: 833335 / min. Contact us through our help center at help.openai.com if you continue to have issues..\n",
      "Retrying langchain.embeddings.openai.embed_with_retry.<locals>._embed_with_retry in 4.0 seconds as it raised RateLimitError: Rate limit reached for default-text-embedding-ada-002 in organization org-RyMWzYr9lGLc1ON8lKEsa30u on tokens per min. Limit: 1000000 / min. Current: 746516 / min. Contact us through our help center at help.openai.com if you continue to have issues..\n"
     ]
    }
   ],
   "source": [
    "# Vectorstore creation\n",
    "\n",
    "from langchain.vectorstores import Chroma\n",
    "from langchain.embeddings import OpenAIEmbeddings\n",
    "\n",
    "embeddings = OpenAIEmbeddings()\n",
    "\n",
    "# vectorstore_elements = Chroma.from_documents(\n",
    "#     collection_name=\"elements\",\n",
    "#     documents = docs_elements + docs_metadata,\n",
    "#     embedding = embeddings,\n",
    "#     persist_directory=\"./vectorstore_elements\"\n",
    "# )\n",
    "\n",
    "# vectorstore_nltk = Chroma.from_documents(\n",
    "#     collection_name=\"nltk\",\n",
    "#     documents = docs_nltk + docs_metadata,\n",
    "#     embedding = embeddings,\n",
    "#     persist_directory=\"./vectorstore_nltk\"\n",
    "# )\n",
    "\n",
    "# vectorstore_recursive = Chroma.from_documents(\n",
    "#     collection_name=\"recursive\",\n",
    "#     documents = docs_recursive + docs_metadata,\n",
    "#     embedding = embeddings,\n",
    "#     persist_directory=\"./vectorstore_recursive\"\n",
    "# )"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.9"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
